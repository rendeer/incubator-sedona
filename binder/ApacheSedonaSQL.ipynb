{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.0.0-incubating,org.datasyslab:geotools-wrapper:geotools-24.0'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry Constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     arealandmark|\n",
      "+-----------------+\n",
      "|POINT (1.1 101.1)|\n",
      "|POINT (2.1 102.1)|\n",
      "|POINT (3.1 103.1)|\n",
      "|POINT (4.1 104.1)|\n",
      "|POINT (5.1 105.1)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as arealandmark from pointtable\")\n",
    "point_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkt_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small.tsv\")\n",
    "\n",
    "polygon_wkt_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromText(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromWKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|            name|         countyshape|\n",
      "+----------------+--------------------+\n",
      "|   Cuming County|POLYGON ((-97.019...|\n",
      "|Wahkiakum County|POLYGON ((-123.43...|\n",
      "|  De Baca County|POLYGON ((-104.56...|\n",
      "|Lancaster County|POLYGON ((-96.910...|\n",
      "| Nuckolls County|POLYGON ((-98.273...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_wkb_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/county_small_wkb.tsv\")\n",
    "\n",
    "polygon_wkb_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select polygontable._c6 as name, ST_GeomFromWKB(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ST_GeomFromGeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         countyshape|\n",
      "+--------------------+\n",
      "|POLYGON ((-87.621...|\n",
      "|POLYGON ((-85.719...|\n",
      "|POLYGON ((-86.000...|\n",
      "|POLYGON ((-86.574...|\n",
      "|POLYGON ((-85.382...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polygon_json_df = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \"\\t\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    load(\"data/testPolygon.json\")\n",
    "\n",
    "polygon_json_df.createOrReplaceTempView(\"polygontable\")\n",
    "polygon_df = spark.sql(\"select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\")\n",
    "polygon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Distance Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "DistanceJoin pointshape1#261: geometry, pointshape2#287: geometry, 2.0, false\n",
      ":- Project [st_point(cast(_c0#255 as decimal(24,20)), cast(_c1#256 as decimal(24,20))) AS pointshape1#261, abc AS name1#262]\n",
      ":  +- FileScan csv [_c0#255,_c1#256] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/jiayu/GitHub/GeoSpark-datasys-repo/binder/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "+- Project [st_point(cast(_c0#281 as decimal(24,20)), cast(_c1#282 as decimal(24,20))) AS pointshape2#287, def AS name2#288]\n",
      "   +- FileScan csv [_c0#281,_c1#282] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/jiayu/GitHub/GeoSpark-datasys-repo/binder/data/testpoint.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string>\n",
      "\n",
      "\n",
      "+-----------------+-----+-----------------+-----+\n",
      "|      pointshape1|name1|      pointshape2|name2|\n",
      "+-----------------+-----+-----------------+-----+\n",
      "|POINT (1.1 101.1)|  abc|POINT (1.1 101.1)|  def|\n",
      "|POINT (1.1 101.1)|  abc|POINT (2.1 102.1)|  def|\n",
      "|POINT (2.1 102.1)|  abc|POINT (1.1 101.1)|  def|\n",
      "|POINT (2.1 102.1)|  abc|POINT (2.1 102.1)|  def|\n",
      "|POINT (2.1 102.1)|  abc|POINT (3.1 103.1)|  def|\n",
      "+-----------------+-----+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "point_csv_df_1 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df_1.createOrReplaceTempView(\"pointtable\")\n",
    "\n",
    "point_df1 = spark.sql(\"SELECT ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape1, \\'abc\\' as name1 from pointtable\")\n",
    "point_df1.createOrReplaceTempView(\"pointdf1\")\n",
    "\n",
    "point_csv_df2 = spark.read.format(\"csv\").\\\n",
    "    option(\"delimiter\", \",\").\\\n",
    "    option(\"header\", \"false\").load(\"data/testpoint.csv\")\n",
    "\n",
    "point_csv_df2.createOrReplaceTempView(\"pointtable\")\n",
    "point_df2 = spark.sql(\"select ST_Point(cast(pointtable._c0 as Decimal(24,20)),cast(pointtable._c1 as Decimal(24,20))) as pointshape2, \\'def\\' as name2 from pointtable\")\n",
    "point_df2.createOrReplaceTempView(\"pointdf2\")\n",
    "\n",
    "distance_join_df = spark.sql(\"select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) < 2\")\n",
    "distance_join_df.explain()\n",
    "distance_join_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Join - Range Join and RDD API Join\n",
    "\n",
    "Please refer to the example - airports per country: https://github.com/apache/incubator-sedona/blob/master/binder/ApacheSedonaSQL_SpatialJoin_AirportsPerCountry.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting GeoPandas to Apache Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"data/gis_osm_pois_free_1.shp\")\n",
    "\n",
    "osm_points = spark.createDataFrame(\n",
    "    gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- osm_id: string (nullable = true)\n",
      " |-- code: long (nullable = true)\n",
      " |-- fclass: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|            geometry|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (15.3393145...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (14.8709625...|\n",
      "|29947493|2402|    motel|          null|POINT (15.0946636...|\n",
      "|29947498|2602|      atm|          null|POINT (15.0732014...|\n",
      "|29947499|2401|    hotel|          null|POINT (15.0696777...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osm_points.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_points.createOrReplaceTempView(\"points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT osm_id,\n",
    "               code,\n",
    "               fclass,\n",
    "               name,\n",
    "               ST_Transform(geometry, 'epsg:4326', 'epsg:2180') as geom \n",
    "        FROM points\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------+--------------------+\n",
      "|  osm_id|code|   fclass|          name|                geom|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "|26860257|2422|camp_site|      de Kroon|POINT (-3288183.3...|\n",
      "|26860294|2406|   chalet|Leśne Ustronie|POINT (-3341183.9...|\n",
      "|29947493|2402|    motel|          null|POINT (-3320466.5...|\n",
      "|29947498|2602|      atm|          null|POINT (-3323205.7...|\n",
      "|29947499|2401|    hotel|          null|POINT (-3323655.1...|\n",
      "+--------+----+---------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.createOrReplaceTempView(\"points_2180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_within_1000m = spark.sql(\"\"\"\n",
    "        SELECT a.osm_id AS id_1,\n",
    "               b.osm_id AS id_2,\n",
    "               a.geom \n",
    "        FROM points_2180 AS a, points_2180 AS b \n",
    "        WHERE ST_Distance(a.geom,b.geom) < 50\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|     id_1|      id_2|                geom|\n",
      "+---------+----------+--------------------+\n",
      "|197624402| 197624402|POINT (-3383818.5...|\n",
      "|197663196| 197663196|POINT (-3383367.1...|\n",
      "|197953474| 197953474|POINT (-3383763.3...|\n",
      "|262310516| 262310516|POINT (-3384257.6...|\n",
      "|262310516|1074233123|POINT (-3384257.6...|\n",
      "|270281140| 270281140|POINT (-3385421.2...|\n",
      "|270281140|1074232906|POINT (-3385421.2...|\n",
      "|270306609| 270306609|POINT (-3383982.8...|\n",
      "|270306746| 270306746|POINT (-3383898.4...|\n",
      "|293896571| 293896571|POINT (-3385029.0...|\n",
      "|293896571|3256728465|POINT (-3385029.0...|\n",
      "|360178884| 360178884|POINT (-3377483.1...|\n",
      "|360178897| 360178897|POINT (-3374350.0...|\n",
      "|360178897| 360178897|POINT (-3374350.0...|\n",
      "|360178897|5546280698|POINT (-3374350.0...|\n",
      "|360178897|5546280699|POINT (-3374350.0...|\n",
      "|360178897| 360178897|POINT (-3374350.0...|\n",
      "|360178897| 360178897|POINT (-3374350.0...|\n",
      "|360178897|5546280698|POINT (-3374350.0...|\n",
      "|360178897|5546280699|POINT (-3374350.0...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neighbours_within_1000m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Apache Sedona to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = neighbours_within_1000m.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df, geometry=\"geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197624402</td>\n",
       "      <td>197624402</td>\n",
       "      <td>POINT (-3383818.580 4179182.169)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197663196</td>\n",
       "      <td>197663196</td>\n",
       "      <td>POINT (-3383367.151 4179427.096)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197953474</td>\n",
       "      <td>197953474</td>\n",
       "      <td>POINT (-3383763.332 4179408.785)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262310516</td>\n",
       "      <td>262310516</td>\n",
       "      <td>POINT (-3384257.682 4178033.053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262310516</td>\n",
       "      <td>1074233123</td>\n",
       "      <td>POINT (-3384257.682 4178033.053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45314</th>\n",
       "      <td>6620325385</td>\n",
       "      <td>6620325385</td>\n",
       "      <td>POINT (-3215183.489 4307887.823)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45315</th>\n",
       "      <td>6631077531</td>\n",
       "      <td>6631077531</td>\n",
       "      <td>POINT (-3227737.389 4306566.622)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45316</th>\n",
       "      <td>6736467188</td>\n",
       "      <td>6736467188</td>\n",
       "      <td>POINT (-3242695.331 4298828.321)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45317</th>\n",
       "      <td>6736772185</td>\n",
       "      <td>6736772185</td>\n",
       "      <td>POINT (-3204857.139 4313763.361)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45318</th>\n",
       "      <td>6817416704</td>\n",
       "      <td>6817416704</td>\n",
       "      <td>POINT (-3214549.268 4314872.904)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45319 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id_1        id_2                              geom\n",
       "0       197624402   197624402  POINT (-3383818.580 4179182.169)\n",
       "1       197663196   197663196  POINT (-3383367.151 4179427.096)\n",
       "2       197953474   197953474  POINT (-3383763.332 4179408.785)\n",
       "3       262310516   262310516  POINT (-3384257.682 4178033.053)\n",
       "4       262310516  1074233123  POINT (-3384257.682 4178033.053)\n",
       "...           ...         ...                               ...\n",
       "45314  6620325385  6620325385  POINT (-3215183.489 4307887.823)\n",
       "45315  6631077531  6631077531  POINT (-3227737.389 4306566.622)\n",
       "45316  6736467188  6736467188  POINT (-3242695.331 4298828.321)\n",
       "45317  6736772185  6736772185  POINT (-3204857.139 4313763.361)\n",
       "45318  6817416704  6817416704  POINT (-3214549.268 4314872.904)\n",
       "\n",
       "[45319 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-sedona",
   "language": "python",
   "name": "apache-sedona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
